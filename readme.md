# ETL CNPJs ‚Äî Profissionais da Constru√ß√£o

> Pipeline ETL em Python + SQLite para processar a base p√∫blica de CNPJs do Brasil e gerar uma base consult√°vel com foco em profissionais e empresas do setor de constru√ß√£o (pintores, eletricistas, encanadores, pedreiros etc.).

---

## ‚úÖ Resumo

Automatiza o que antes era feito manualmente: baixar, transformar e carregar os dados p√∫blicos (empresas, estabelecimentos, CNAE, munic√≠pios, situa√ß√£o cadastral) em um banco SQLite otimizado para consultas. O projeto foca em: desempenho (processamento em *chunks*), qualidade de dados e cria√ß√£o de conjuntos filtrados para prospec√ß√£o comercial.

> **Observa√ß√£o importante:** √© relevante e leg√≠timo mencionar que voc√™ contou com **IA como apoio** (copiloto) no desenvolvimento ‚Äî principalmente para validar trechos de c√≥digo, gerar regex/CNAE mappings e ganhar produtividade. Isso mostra maturidade t√©cnica e uso de ferramentas modernas.

---

## üì¶ Tecnologias

- Python 3.9+
- Pandas
- SQLite (biblioteca `sqlite3` ou SQLAlchemy)
- tqdm (barra de progresso)
- requests (para download dos arquivos oficiais)

Arquivo `requirements.txt` sugerido:

```
pandas
sqlalchemy
tqdm
requests
python-dateutil
```

---

## üìÅ Estrutura do reposit√≥rio (sugest√£o)

```
etl-cnpjs/
‚îú‚îÄ etl/
‚îÇ  ‚îú‚îÄ download.py
‚îÇ  ‚îú‚îÄ transform.py
‚îÇ  ‚îú‚îÄ load.py
‚îÇ  ‚îî‚îÄ helpers.py
‚îú‚îÄ notebooks/            # an√°lises e EDA
‚îú‚îÄ sql/                  # queries reutiliz√°veis
‚îú‚îÄ data/                 # *N√ÉO* com dados sens√≠veis grandes (adicionar no .gitignore)
‚îú‚îÄ README.md
‚îú‚îÄ requirements.txt
‚îî‚îÄ .gitignore
```

---

## üîß Quickstart ‚Äî rodar localmente

1. Crie e ative um virtualenv:

```bash
python -m venv venv
# Windows
venv\Scripts\activate
# macOS / Linux
source venv/bin/activate
```

2. Instale depend√™ncias:

```bash
pip install -r requirements.txt
```

3. Execute o ETL (exemplo):

```bash
python etl/download.py   # baixa os arquivos oficiais (opcional)
python etl/load.py       # processa em chunks e carrega no SQLite
```

> **Importante:** n√£o versionar a base inteira no Git (20 GB). Mantenha `data/` no `.gitignore` e compartilhe amostras ou scripts de rebuild.

---

## ‚öôÔ∏è Detalhe t√©cnico: _chunking_ e performance

### Por que processar em *chunks*?

- As fontes s√£o muito grandes (ex.: tens de GB). Ler tudo na mem√≥ria causa swap e torna a m√°quina inutiliz√°vel.
- Processar em blocos permite transformar e gravar incrementalmente.

### Estrat√©gia pr√°tica

- Use `pandas.read_csv(..., chunksize=CHUNK_SIZE, compression='gzip')` para iterar sobre o CSV compactado.
- Defina `CHUNK_SIZE` conforme RAM dispon√≠vel. Recomendo come√ßar com **CHUNK_SIZE = 50_000** e testar; se ficar est√°vel, aumentar para 100_000.
- Sempre selecione apenas as colunas necess√°rias com `usecols=[...]` e force tipos (`dtype=...`) para reduzir uso de mem√≥ria.
- Use `df.astype({'col': 'category'})` para colunas com baixa cardinalidade (ex.: c√≥digos de situa√ß√£o).

### PRAGMAS SQLite para acelerar cargas

Execute estes PRAGMA antes de cargas massivas (e volte ao padr√£o depois, se desejar):

```sql
PRAGMA journal_mode = WAL;
PRAGMA synchronous = NORMAL;   -- ou OFF para maior velocidade (risco de corrup√ß√£o em queda de energia)
PRAGMA temp_store = MEMORY;
PRAGMA cache_size = -200000;   -- cache em p√°ginas (~ ajusta conforme necessidade)
```

- **Dica:** crie √≠ndices **ap√≥s** carregar os dados (indexa√ß√£o durante inserts torna a carga muito mais lenta).

### Exemplo: carregamento chunked r√°pido (sqlite3 + executemany)

```python
# etl/load_chunked.py
import sqlite3
import pandas as pd
from tqdm import tqdm

DB = 'data/cnpjs.db'
CSV = 'data/estabelecimentos.csv.gz'
CHUNK_SIZE = 50_000
BATCH_INSERT = 5000

def set_pragmas(conn):
    cur = conn.cursor()
    cur.execute('PRAGMA journal_mode=WAL;')
    cur.execute('PRAGMA synchronous=NORMAL;')
    cur.execute('PRAGMA temp_store=MEMORY;')
    cur.execute('PRAGMA cache_size=-200000;')
    conn.commit()

def process_chunk(df):
    cols = ['cnpj_basico','id_municipio','id_cnae','situacao_cadastral']
    df = df[cols].copy()
    df['cnpj_basico'] = df['cnpj_basico'].astype(str).str.zfill(14)
    # outras transforma√ß√µes/valida√ß√µes
    return df


def chunked_load():
    conn = sqlite3.connect(DB, timeout=30)
    set_pragmas(conn)
    cur = conn.cursor()

    # certifique-se que a tabela existe (create table if not exists ...)

    reader = pd.read_csv(CSV, compression='gzip', sep=';', chunksize=CHUNK_SIZE, dtype=str, usecols=['cnpj_basico','id_municipio','id_cnae','situacao_cadastral'], low_memory=False)
    for chunk in tqdm(reader):
        df = process_chunk(chunk)
        rows = [tuple(x) for x in df.to_numpy()]
        for i in range(0, len(rows), BATCH_INSERT):
            batch = rows[i:i+BATCH_INSERT]
            cur.executemany('INSERT INTO estabelecimentos (cnpj_basico, id_municipio, id_cnae, situacao_cadastral) VALUES (?,?,?,?)', batch)
            conn.commit()
    conn.close()

if __name__ == '__main__':
    chunked_load()
```

### Alternativa: `pandas.to_sql` com SQLAlchemy (mais simples)

```python
from sqlalchemy import create_engine
engine = create_engine('sqlite:///data/cnpjs.db', connect_args={{'timeout': 30}})
for chunk in pd.read_csv(..., chunksize=CHUNK_SIZE):
    chunk = process_chunk(chunk)
    chunk.to_sql('estabelecimentos', engine, if_exists='append', index=False, method='multi', chunksize=5000)
```

> `to_sql(..., method='multi')` envia pacotes multi-row e costuma ser mais r√°pido que inserts individuais quando a camada SQLAlchemy est√° bem configurada.

---

## üèÅ P√≥s-processamento e otimiza√ß√µes

- **Criar √≠ndices**:

```sql
CREATE INDEX idx_est_cnpj ON estabelecimentos(cnpj_basico);
CREATE INDEX idx_est_cnae ON estabelecimentos(id_cnae);
CREATE INDEX idx_est_mun ON estabelecimentos(id_municipio);
```

- Rodar `ANALYZE;` para ajudar o otimizador de queries.
- Se o DB final ficar com muito espa√ßo livre, rodar `VACUUM;` (pode demorar).

---

## üîç Valida√ß√µes e QA (inclui uso de IA)

- Validar contagens por tabela entre arquivo original e DB carregado.
- Verificar valores nulos e anomalias por CNAE e munic√≠pio.
- Gerar amostras estratificadas para revis√£o manual.
- **IA como apoio**: gerar express√µes regulares, revisar snippets de SQL/Python, sugerir mapeamentos CNAE ‚Üí categoria (ex.: agrupar sub-classes de atividades de constru√ß√£o). Mencione isso no README ‚Äî √© leg√≠timo e agrega valor.

---

## üí° Insights que a base oferece (para neg√≥cio)

- **Lojistas de materiais**: identificar profissionais ativos por regi√£o e CNAE para campanhas locais e promo√ß√£o de kits (ex.: kit pintura, kit el√©trica).
- **Fornecedores/ind√∫strias**: montar listas segmentadas por CNAE e situa√ß√£o cadastral para a√ß√µes comerciais B2B.
- **Marketplace / Plataformas de servi√ßos**: entender densidade de profissionais por munic√≠pio para priorizar expans√£o.

---

